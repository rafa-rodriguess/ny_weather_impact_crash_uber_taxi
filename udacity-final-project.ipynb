{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1609527416191_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-2-201.us-west-2.compute.internal:20888/proxy/application_1609527416191_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-8-99.us-west-2.compute.internal:8042/node/containerlogs/container_1609527416191_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import configparser\n",
    "import requests\n",
    "import boto3\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=\"AKKHU4FVNX\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=\"Ur9oXp0fY9/80s3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbcb302ff054f2693567f6a90fd713f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Creates the spark session used in all the code\n",
    "\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Udacity Final Project\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf86b698647c4202bae6164505e55438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_csv(sparksession, path=[], header=True, headerList=[]):    \n",
    "    if ',' in path:\n",
    "        path = path.split(',')\n",
    "    \n",
    "    if header:\n",
    "        df = sparksession.read.options(header='True',inferSchema='True').csv(path)\n",
    "    else:      \n",
    "        df = sparksession.read.options(header='False',inferSchema='True').csv(path).toDF(*headerList)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7cfb6d0f2549a889bb6efc291934fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_JSON(sparksession, path=[]): \n",
    "    \n",
    "    df = sparksession.read.option(\"multiline\", \"true\").json(path)\n",
    "    \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60efff8fae9e4f8290c0012ddeffda49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def readConfigFromURL(url):    \n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read_string(r.text)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07b0e0eea864299b4e3f23686ae1624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def S3Exists(S3BucketAndKey):\n",
    "    s3 = boto3.resource('s3')\n",
    "    match = re.match(r's3:\\/\\/(.+?)\\/(.+)', S3BucketAndKey)\n",
    "    bucket = s3.Bucket(match.group(1))\n",
    "    s3file = match.group(2)\n",
    "    objs = list(bucket.objects.filter(Prefix=s3file))\n",
    "    return any([w.key == s3file for w in objs])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c696201e188401388375feaec479171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataIngestionS3toS3(S3Dest, listOfOrign=[], preKey=\"\", overWrite=False):\n",
    "    s3 = boto3.resource('s3')\n",
    "    return_list = []    \n",
    "    \n",
    "    for l in listOfOrign:\n",
    "        match = re.match(r's3:\\/\\/(.+?)\\/(.+)', l)\n",
    "        copy_source = {\n",
    "            'Bucket': match.group(1),\n",
    "            'Key': match.group(2)\n",
    "         }\n",
    "        \n",
    "        destFile = preKey + \"/\" + copy_source[\"Key\"]        \n",
    "       \n",
    "        \n",
    "        if S3Exists(S3Dest + \"/\" + destFile)==False or overWrite:\n",
    "            s3.meta.client.copy(copy_source, S3Dest.replace(\"s3://\",\"\"), destFile)\n",
    "\n",
    "        return_list.append(S3Dest + \"/\" + destFile)\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f777dd594e648728f75904d074d5c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataIngestionHTTPtoS3(S3Dest, listOfOrign=[], preKey=\"\",overWrite=False):\n",
    "    s3 = boto3.client('s3')\n",
    "    return_list = []    \n",
    "    \n",
    "    for l in listOfOrign:\n",
    "        req = requests.get(l)\n",
    "        req_data = [line.decode() + '\\n' for line in req.iter_lines()]\n",
    "        req_data = \"\".join(req_data)\n",
    "        \n",
    "        \n",
    "        if 'content-disposition' in req.headers:\n",
    "            d = req.headers['content-disposition']\n",
    "            filename = re.findall(\"filename=(.+)\", d)[0]\n",
    "            filename = filename.replace('\"','')\n",
    "        else:\n",
    "            filename = l.split(\"/\")[-1]               \n",
    "\n",
    "        destFile = preKey + \"/\" + filename\n",
    " \n",
    "        \n",
    "        if S3Exists(S3Dest + \"/\" + destFile)==False or overWrite:\n",
    "            s3.put_object(Bucket=S3Dest.replace(\"s3://\",\"\"), Key=destFile, Body=req_data)\n",
    "    \n",
    "        return_list.append(S3Dest + \"/\" + destFile)\n",
    "        \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ae85bd8c084c6e9d0be52beab01a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def saveDFtoParquetS3(df, S3Dest, partitionBy=[], overWrite=False):\n",
    "    if overWrite or S3Exists(S3Dest) == False:\n",
    "        df.write.parquet(S3Dest, partitionBy=partitionBy, mode='Overwrite')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb9fdc240b9444786c2d629edc29f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataQuality(df):\n",
    "    #corrects any column name\n",
    "    newDf= df.toDF(*(re.sub(r'[\\.\\s]+', '_', c) for c in df.columns))    \n",
    "        \n",
    "    #take off any empty rows\n",
    "    newDf = newDf.dropna(how=\"all\") \n",
    "    \n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4423c366b0b44785928ad290c4f933d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataPrepGreenCabDF(df):\n",
    "    #Data Dquality\n",
    "    newDf = dataQuality(df)\n",
    "  \n",
    "    \n",
    "    #Select only the columns we are going to use\n",
    "    #cast datetime\n",
    "    #2014-04-01 00:00:00\n",
    "    newDf = newDf.select(F.to_timestamp(newDf[\"lpep_pickup_datetime\"], \"yyyy-MM-dd HH:mm:ss\").alias(\"DT\"))\n",
    "    newDf = newDf.select(\"DT\", F.year(\"DT\").alias('Year'), F.month(\"DT\").alias('Month'), F.dayofmonth(\"DT\").alias('Day'), F.dayofweek(\"DT\").alias(\"Day_Of_Week\"), F.hour(newDf[\"DT\"]).alias(\"Hour\"), F.minute(newDf[\"DT\"]).alias(\"Minute\"), F.lit(\"Green Cab\").alias(\"type\"))\n",
    "    \n",
    "       \n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bf7877a1b54dfeb5d9e1377acfc261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataPrepYellowCabDF(df):\n",
    "    #Data Dquality\n",
    "    newDf = dataQuality(df)\n",
    "        \n",
    "    #Select only the columns we are going to use\n",
    "    #cast datetime    \n",
    "    newDf = newDf.select(F.to_timestamp(newDf[\"_pickup_datetime\"], \"yyyy-MM-dd HH:mm:ss\").alias(\"DT\"))\n",
    "    newDf = newDf.select(\"DT\", F.year(\"DT\").alias('Year'), F.month(\"DT\").alias('Month'), F.dayofmonth(\"DT\").alias('Day'), F.dayofweek(\"DT\").alias(\"Day_Of_Week\"), F.hour(newDf[\"DT\"]).alias(\"Hour\"), F.minute(newDf[\"DT\"]).alias(\"Minute\"), F.lit(\"Yellow Cab\").alias(\"type\"))\n",
    "    \n",
    "    \n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53c68d029214b40aa94b2fc7852bb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataPrepUber(df):\n",
    "    #Data Dquality\n",
    "    newDf = dataQuality(df)\n",
    "    \n",
    "      \n",
    "    #Select only the columns we are going to use\n",
    "    #cast datetime\n",
    "    if \"Pickup_date\" in newDf.columns:\n",
    "        date_field = \"Pickup_date\"  \n",
    "        date_format = \"yyyy-MM-dd HH:mm:ss\"\n",
    "    else:\n",
    "        date_field = \"Date/Time\"\n",
    "        date_format = \"MM/dd/yyyy HH:mm:ss\"\n",
    "    \n",
    "    \n",
    "    newDf = newDf.select(F.to_timestamp(newDf[date_field], date_format).alias(\"DT\"))        \n",
    "    newDf = newDf.select(\"DT\", F.year(\"DT\").alias('Year'), F.month(\"DT\").alias('Month'), F.dayofmonth(\"DT\").alias('Day'), F.dayofweek(\"DT\").alias(\"Day_Of_Week\"), F.hour(newDf[\"DT\"]).alias(\"Hour\"), F.minute(newDf[\"DT\"]).alias(\"Minute\"), F.lit(\"Uber\").alias(\"type\"))\n",
    "    \n",
    "    \n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8bd2e6da2641d4be93b3e788d435f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataPrepWeather(df):\n",
    "    #Data Dquality\n",
    "    newDf = dataQuality(df)    \n",
    "\n",
    "    newDf = newDf.select(F.to_timestamp(F.from_unixtime(F.unix_timestamp(F.concat(F.to_date(newDf[\"Date\"]), F.lit(\" \"), newDf[\"Time\"]),'yyyy-MM-dd hh:mm aa'),\"MM/dd/yyyy HH:mm\"),\"MM/dd/yyyy HH:mm\").alias(\"DT\"), \"Condition\", \"Station_Name\")    \n",
    "    newDf = newDf.select(\"DT\", F.year(\"DT\").alias('Year'), F.month(\"DT\").alias('Month'), F.dayofmonth(\"DT\").alias('Day'), F.dayofweek(\"DT\").alias(\"Day_Of_Week\"), F.hour(newDf[\"DT\"]).alias(\"Hour\"), \"Condition\", \"Station_Name\")    \n",
    "    \n",
    "    #DATA QUALITY - There are more than 1 weather condition on the sme hour. Take the first one\n",
    "    window = Window.partitionBy(\"Year\", \"Month\", \"Day\", \"Hour\", \"Station_Name\").orderBy(\"DT\")\n",
    "    newDf = newDf.withColumn(\"row_number\",F.row_number().over(window))\n",
    "    newDf=  newDf.filter(newDf[\"row_number\"]==1).drop(\"DT\", \"row_number\")\n",
    "     \n",
    "    return newDf\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1614f27324814688ad508cd1ab714b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataPrepCrash(dF):\n",
    "    #Data Dquality\n",
    "    newDf = dataQuality(dF)\n",
    "    \n",
    "    newDf = newDf.withColumn(\"DT\",F.to_timestamp(F.concat(F.to_date(F.to_timestamp(F.col(\"CRASH_DATE\"))),F.lit(\" \"), F.col(\"CRASH_TIME\")), \"yyyy-MM-dd HH:mm\") )\n",
    "    \n",
    "    newDf = newDf.select(\"DT\",\n",
    "                         F.year(\"CRASH_DATE\").alias(\"Year\"), \\\n",
    "                         F.month(\"CRASH_DATE\").alias(\"Month\"), \\\n",
    "                         F.dayofmonth(\"CRASH_DATE\").alias(\"Day\"), \\\n",
    "                         F.dayofweek(\"CRASH_DATE\").alias(\"Day_Of_Week\"), \\\n",
    "                         F.hour(\"CRASH_TIME\").alias(\"Hour\"), \\\n",
    "                         F.minute(\"CRASH_TIME\").alias(\"Minute\"))\n",
    "    newDf = newDf.filter(newDf[\"Year\"] != 2020)\n",
    "    \n",
    "    return newDf\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657a245612fa48068f2ccfd1d02914d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mergeDataFrames(iniDf, DataFrame=[]):\n",
    "    for d in DataFrame:\n",
    "        iniDf.union(d)\n",
    "    return iniDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029c0965cad749988a01ce527d38b63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataTransformation(spark, config, dataLoadedDictDF):\n",
    "    \"\"\"   \n",
    "    - DATA TRANSFORMATION\n",
    "    \n",
    "    - Loads data from the Blob Area\n",
    "    \n",
    "    - Do some Quality Checks and Save the transformed files into de DATA STORAGE Area, in parquet format\n",
    "    \n",
    "    - Creates the final Analysis Dataframes and stores in the DATA STORAGE\n",
    "    \"\"\"\n",
    "    #Data Cleansing / Data Prep\n",
    "    yellowCab = dataPrepYellowCabDF(dataLoadedDictDF[\"yellowCab\"]) \n",
    "    greenCab  = dataPrepGreenCabDF(dataLoadedDictDF[\"greenCab\"])    \n",
    "    uber14    = dataPrepUber(dataLoadedDictDF[\"uber14\"])    \n",
    "    uber15    = dataPrepUber(dataLoadedDictDF[\"uber15\"])    \n",
    "    weather   = dataPrepWeather(dataLoadedDictDF[\"weather\"])    \n",
    "    crash     = dataPrepCrash(dataLoadedDictDF[\"crash\"])\n",
    "          \n",
    "    \n",
    "    storage = config.get('DATASTORAGE','DATASTORAGE')\n",
    "    storageFolder = config.get('DATASTORAGE','DATASTORAGE_FOLDER')\n",
    "\n",
    "    #Saves Data to Data Storage\n",
    "    saveDFtoParquetS3(yellowCab, storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','YELLOWCABPARQUET'), [\"Year\", \"Month\"])\n",
    "    saveDFtoParquetS3(greenCab, storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','GREENCABPARQUET'), [\"Year\", \"Month\"])\n",
    "    saveDFtoParquetS3(uber14, storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','UBER14PARQUET'), [\"Year\", \"Month\"])\n",
    "    saveDFtoParquetS3(uber15, storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','UBER15PARQUET'), [\"Year\", \"Month\"])\n",
    "    saveDFtoParquetS3(weather, storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','WEATHERPARQUET'), [\"Year\", \"Month\"])\n",
    "    saveDFtoParquetS3(crash, storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','CRASHPARQUET'), [\"Year\", \"Month\"])\n",
    " \n",
    "    #CREATES FINAL DATAFRAMES FOR ANALYSIS\n",
    "    ## PICKUPS DATAFRAME FOR ANALYSIS\n",
    "    allpickups = mergeDataFrames(yellowCab, [greenCab,uber14, uber15])        \n",
    "   \n",
    "    ## SAVES MERGED PICKUPS DATAFRAME TO DATASTORAGE\n",
    "    saveDFtoParquetS3(allpickups, storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','ALLPICKUPSPARQUET'), [\"Year\", \"Month\"])\n",
    "       \n",
    "    \n",
    "    ##JOIN DATA QUALIT. COLLECT ORIGINAL ROWS COUNT    \n",
    "    dtQualityBeforeJoinRowsCount = allpickups.count()\n",
    "\n",
    "    start5 = timer()\n",
    "    station01 = weather.filter(weather[\"Station_Name\"] == \"John F. Kennedy International Airport Station\").dropDuplicates()\n",
    "    station02 = weather.filter(weather[\"Station_Name\"] == \"LaGuardia Airport Station\").dropDuplicates()\n",
    "\n",
    "    \n",
    "    analysisPickupsDf = allpickups.join(station01, on=[\"Year\",\"Month\",\"Day\",\"Hour\"],how='left')\\\n",
    "                                         .join(station02, on=[\"Year\",\"Month\",\"Day\",\"Hour\"],how='left')\\\n",
    "                                         .select(allpickups['*'],station01['Condition'].alias(\"JFK_Condition\"),\\\n",
    "                                             station02['Condition'].alias(\"LGA_Condition\"))\n",
    " \n",
    "    \n",
    "    ##COLLECT ROWS COUNT AFTER JOIN\n",
    "    ##ROWS BEFORE AND AFTER JOIN MUST BE THE SAME\n",
    "    ##SAVES DATA DO DATA STORAGE    \n",
    "    dtQualityAfterJoinRowsCount = analysisPickupsDf.count()\n",
    "    if dtQualityBeforeJoinRowsCount == dtQualityAfterJoinRowsCount:\n",
    "        saveDFtoParquetS3(analysisPickupsDf, storage + \"//\" + storageFolder + \"/\" + config.get('DATASTORAGE','ANALYSISPICKUPSPARQUET'), [\"Year\", \"Month\"])\n",
    "    else:\n",
    "        print(\"Error joing tables. Before joing dataframe {} had {} rows. After {}\".format(\"analysisPickupsDf\", dtQualityBeforeJoinRowsCount, dtQualityAfterJoinRowsCount))\n",
    "\n",
    "    ## CRASH DATAFRAME FOR ANALYSIS\n",
    "    analysisCrashsDf = crash        \n",
    "\n",
    "    ##JOIN DATA QUALIT. COLLECT ORIGINAL ROWS COUNT\n",
    "    dtQualityBeforeJoinRowsCount = analysisCrashsDf.count()\n",
    "    start6 = timer()\n",
    "    analysisCrashsDf = analysisCrashsDf.join(station01, on=[\"Year\",\"Month\",\"Day\",\"Hour\"],how='left')\\\n",
    "                                       .join(station02, on=[\"Year\",\"Month\",\"Day\",\"Hour\"],how='left')\\\n",
    "                                       .select(analysisCrashsDf['*'],station01['Condition'].alias(\"JFK_Condition\"),\\\n",
    "                                             station02['Condition'].alias(\"LGA_Condition\"))\n",
    "  \n",
    "    \n",
    "    ##COLLECT ROWS COUNT AFTER JOIN\n",
    "    ##ROWS BEFORE AND AFTER JOIN MUST BE THE SAME\n",
    "    ##SAVES DATA DO DATA STORAGE\n",
    "    dtQualityAfterJoinRowsCount = analysisCrashsDf.count()\n",
    "    if dtQualityBeforeJoinRowsCount == dtQualityAfterJoinRowsCount:\n",
    "        saveDFtoParquetS3(analysisCrashsDf, storage + \"//\" + storageFolder + \"/\" + config.get('DATASTORAGE','ANALYSISCRASHSPARQUET'), [\"Year\", \"Month\"])\n",
    "    else:\n",
    "        print(\"Error joing tables. Before joing dataframe {} had {} rows. After {}\".format(\"analysisCrashsDf\", dtQualityBeforeJoinRowsCount, dtQualityAfterJoinRowsCount))\n",
    "\n",
    "       \n",
    "    return {\"yellowCab\" : yellowCab,\n",
    "            \"greenCab\": greenCab,\n",
    "            \"uber14\": uber14,\n",
    "            \"uber15\": uber15,\n",
    "            \"weather\": weather,\n",
    "            \"crash\": crash,\n",
    "            \"analysisPickupsDf\": analysisPickupsDf,\n",
    "            \"analysisCrashsDf\": analysisCrashsDf,\n",
    "            \"allpickups\": allpickups\n",
    "       }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f5bcc20a9947cfb475d5fe26f5d924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataExtration(spark, config, newExtraction=True):\n",
    "    \"\"\"   \n",
    "    - DATA Extraction\n",
    "    \n",
    "    - Ingest all the data to STORAGE BLOB\n",
    "    \n",
    "    - Ingestions from S3 (csv files) and a JSON (API Endpoint)\n",
    "    \n",
    "    - If newExtraction is set to True, do ingestion despite the fact that we alrealy have the file in the STORAGE BLOB\n",
    "      otherwise return the STORAGE BLOB file path\n",
    "    \n",
    "    \"\"\"    \n",
    "    storageBlob =  config.get('STORAGE_BLOB','STORAGE_BLOB')\n",
    "    storageBlobFolder = config.get('STORAGE_BLOB','STORAGE_BLOB_FOLDER')\n",
    "\n",
    "    yellowCabIngestedData = dataIngestionS3toS3(storageBlob, config.get('DATA_SOURCES','YELLOW_CAB').split(\",\"), storageBlobFolder, newExtraction)\n",
    "    greenCabIngestedData  = dataIngestionS3toS3(storageBlob, config.get('DATA_SOURCES','GREEN_CAB').split(\",\"), storageBlobFolder, newExtraction)\n",
    "    uber14IngestedData    = dataIngestionHTTPtoS3(storageBlob, config.get('DATA_SOURCES','UBER14').split(\",\"), storageBlobFolder, newExtraction)\n",
    "    uber15IngestedData    = dataIngestionHTTPtoS3(storageBlob, config.get('DATA_SOURCES','UBER15').split(\",\"), storageBlobFolder, newExtraction)\n",
    "    weatherIngestedData   = dataIngestionHTTPtoS3(storageBlob, config.get('DATA_SOURCES','WEATHER_WUNDERGROUND').split(\",\"), storageBlobFolder,  newExtraction)\n",
    "    crashIngestedData     = dataIngestionHTTPtoS3(storageBlob, config.get('DATA_SOURCES','CRASH').split(\",\"), storageBlobFolder, newExtraction)\n",
    "    \n",
    "    \n",
    "    return {\"yellowCabIngestedData\" : yellowCabIngestedData,\n",
    "            \"greenCabIngestedData\": greenCabIngestedData,\n",
    "            \"uber14IngestedData\": uber14IngestedData,\n",
    "            \"uber15IngestedData\": uber15IngestedData,\n",
    "            \"weatherIngestedData\": weatherIngestedData,\n",
    "            \"crashIngestedData\": crashIngestedData\n",
    "           }\n",
    "    end = timer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f53c09f1d80458cbfd9e614c8e6f5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataLoadFromParquet(parquetFile):\n",
    "    schema = StructType([StructField('none', StringType(), True)])\n",
    "    return spark.read.parquet(parquetFile) if S3Exists(parquetFile+\"/_SUCCESS\") else spark.createDataFrame([], schema)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08eab4d5906a4d2e91c61166eee3486a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataLoadFromDataStorage(spark, config):\n",
    "    \"\"\"   \n",
    "    - Loads Transformed Data From DATA STORAGE   \n",
    "  \n",
    "    \"\"\"\n",
    "    storageBlob =  config.get('STORAGE_BLOB','STORAGE_BLOB')\n",
    "    storageBlobFolder = config.get('STORAGE_BLOB','STORAGE_BLOB_FOLDER')        \n",
    "    storage = config.get('DATASTORAGE','DATASTORAGE')\n",
    "    storageFolder = config.get('DATASTORAGE','DATASTORAGE_FOLDER')\n",
    "    \n",
    "    #Get the already transformed parquet data from the Storage\n",
    "    yellowCab =         dataLoadFromParquet(storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','YELLOWCABPARQUET')) \n",
    "    greenCab =          dataLoadFromParquet(storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','GREENCABPARQUET')) \n",
    "    uber14 =            dataLoadFromParquet(storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','UBER14PARQUET')) \n",
    "    uber15 =            dataLoadFromParquet(storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','UBER15PARQUET')) \n",
    "    weather =           dataLoadFromParquet(storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','WEATHERPARQUET')) \n",
    "    crash =             dataLoadFromParquet(storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','CRASHPARQUET')) \n",
    "    analysisPickupsDf = dataLoadFromParquet(storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','ANALYSISPICKUPSPARQUET')) \n",
    "    analysisCrashsDf =  dataLoadFromParquet(storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','ANALYSISCRASHSPARQUET')) \n",
    "    allpickups =        dataLoadFromParquet(storage + \"/\" + storageFolder + \"/\" + config.get('DATASTORAGE','ALLPICKUPSPARQUET')) \n",
    "    \n",
    "    \n",
    "    return {\"yellowCab\" : yellowCab,\n",
    "        \"greenCab\": greenCab,\n",
    "        \"uber14\": uber14,\n",
    "        \"uber15\": uber15,\n",
    "        \"weather\": weather,\n",
    "        \"crash\": crash,\n",
    "        \"analysisPickupsDf\": analysisPickupsDf,\n",
    "        \"analysisCrashsDf\": analysisCrashsDf,\n",
    "        \"allpickups\": allpickups\n",
    "   }\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79fce9098bd407cb1b844cf94300dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataLoad(spark, config, dataExtractedDict):\n",
    "    \"\"\"   \n",
    "    - Loads Data From STORAGE BLOB into memory   \n",
    "  \n",
    "    \"\"\"\n",
    "    #Load Data in memory from STORE BLOB\n",
    "    yellowCab = load_csv(spark, dataExtractedDict[\"yellowCabIngestedData\"])\n",
    "    greenCab = load_csv(spark, dataExtractedDict[\"greenCabIngestedData\"])\n",
    "    uber14 = load_csv(spark, dataExtractedDict[\"uber14IngestedData\"])     \n",
    "    uber15 = load_csv(spark, dataExtractedDict[\"uber15IngestedData\"])     \n",
    "    weather = load_csv(spark, dataExtractedDict[\"weatherIngestedData\"])\n",
    "    crash = load_JSON(spark, dataExtractedDict[\"crashIngestedData\"])\n",
    "   \n",
    "    return {\"yellowCab\" : yellowCab,\n",
    "        \"greenCab\": greenCab,\n",
    "        \"uber14\": uber14,\n",
    "        \"uber15\": uber15,\n",
    "        \"weather\": weather,\n",
    "        \"crash\": crash\n",
    "       }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657a2e4d8ee140708216450efc1c8d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"   \n",
    "    - Gets the config info from confog.cfg\n",
    "    \n",
    "    - Creates Spark Session\n",
    "    \n",
    "    - Loads Data from S3 Bucket and saves it to another S3 Bucket\n",
    "    \n",
    "    \"\"\"\n",
    "    start_total = timer()\n",
    "    \n",
    "    #loads config    \n",
    "    config = configparser.ConfigParser()\n",
    "    config = read(\"config.cfg\")\n",
    "       \n",
    "    #Creates Spark Session\n",
    "    spark = create_spark_session()\n",
    "  \n",
    "    #EXTRACTION (Data Sourcer --> Storage BLOB)\n",
    "    start = timer()\n",
    "    dataExtractedDict = dataExtration(spark, config, config.get('GENERAL','RUN_EXTRATION') == \"True\")\n",
    "    end = timer()\n",
    "    print(\"dataExtration: \",timedelta(seconds=end-start))\n",
    "       \n",
    "    \n",
    "    #LOAD (Storage BLOB --> Memory)\n",
    "    start = timer()\n",
    "    dataLoadedDictDF = dataLoad(spark, config, dataExtractedDict) if config.get('GENERAL','RUN_LOAD') == \"True\" else {}\n",
    "    end = timer()\n",
    "    print(\"dataLoad: \",timedelta(seconds=end-start))\n",
    "    \n",
    "    #TRANSFORMATION\n",
    "    #If transformation is set to True and Data is Loaded into memory, run transformation\n",
    "    #Load final dataframes otherwise\n",
    "    start = timer()\n",
    "    transformation_condition = config.get('GENERAL','RUN_TRANSFORMATION') == \"True\" and config.get('GENERAL','RUN_LOAD') == \"True\"\n",
    "    dataTransformedDictDf = dataTransformation(spark, config, dataLoadedDictDF) if transformation_condition else dataLoadFromDataStorage(spark, config)\n",
    "    end = timer()\n",
    "    print(\"dataTransformation: \",timedelta(seconds=end-start))\n",
    "    \n",
    "    #Show dataframes created for analysis\n",
    "    dataTransformedDictDf[\"analysisPickupsDf\"].printSchema()       \n",
    "    dataTransformedDictDf[\"analysisCrashsDf\"].printSchema()\n",
    "    \n",
    "    end_total = timer()\n",
    "    print(\"MAIN - Execution Total Time: \",timedelta(seconds=end_total-start_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c259d928a347a093c2e6939f43f9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataExtration:  0:19:12.580805\n",
      "dataLoad:  0:03:26.804214\n",
      "dataTransformation:  0:18:29.819729\n",
      "root\n",
      " |-- DT: timestamp (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Day_Of_Week: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- type: string (nullable = false)\n",
      " |-- JFK_Condition: string (nullable = true)\n",
      " |-- LGA_Condition: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DT: timestamp (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Day_Of_Week: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- JFK_Condition: string (nullable = true)\n",
      " |-- LGA_Condition: string (nullable = true)\n",
      "\n",
      "MAIN - Execution Total Time:  0:41:10.022905"
     ]
    }
   ],
   "source": [
    "if __name__ in (\"__main__\",\"builtins\"):\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
